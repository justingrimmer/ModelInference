\documentclass{beamer}

%\usepackage[table]{xcolor}
\mode<presentation> {
  \usetheme{Boadilla}
%  \usetheme{Pittsburgh}
%\usefonttheme[2]{sans}
\renewcommand{\familydefault}{cmss}
%\usepackage{lmodern}
%\usepackage[T1]{fontenc}
%\usepackage{palatino}
%\usepackage{cmbright}
  \setbeamercovered{transparent}
\useinnertheme{rectangles}
}
%\usepackage{normalem}{ulem}
%\usepackage{colortbl, textcomp}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{structure}{fg= black}
\definecolor{trial}{cmyk}{1,0,0, 0}
\definecolor{trial2}{cmyk}{0.00,0,1, 0}
\definecolor{darkgreen}{rgb}{0,.4, 0.1}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma} 

\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif}

\beamertemplatesolidbackgroundcolor{white}  \setbeamercolor{alerted
text}{fg=red}
\usepackage{multirow}

\setbeamertemplate{caption}[numbered]\newcounter{mylastframe}

%\begin{beamerboxesrounded}[upper=uppercol,lower=lowercol,shadow=true]{Block}
%$A = B$.
%\end{beamerboxesrounded}}
\renewcommand{\familydefault}{cmss}
\usepackage[all]{xy}
\usepackage{tikz}
\usetikzlibrary{arrows}
%\usepackage{palatino}
%\usepackage{eulervm}
\usecolortheme{lily}
%\usepackage[latin1]{inputenc}

\title[Methodology III] % (optional, nur bei langen Titeln n√∂tig)
{Political Methodology III: Model Based Inference}

\author{Justin Grimmer}
\institute[Stanford University]{Associate Professor\\Department of Political Science \\  Stanford University}
\vspace{0.3in}

\date{April 19th, 2017}

\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}


\begin{eqnarray}
\frac{\partial l(\boldsymbol{\theta}| \boldsymbol{Y})  }{\partial \theta_{j} } & = & \frac{1}{f(\boldsymbol{Y} | \boldsymbol{\theta})} \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{j} } \nonumber \\
E\left[\frac{\partial l(\boldsymbol{\theta}| \boldsymbol{Y})  }{\partial \theta_{j} } \right] & = & \int  \frac{1}{f(\boldsymbol{Y} | \boldsymbol{\theta})} \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{j} } f(\boldsymbol{Y} | \boldsymbol{\theta}) d\boldsymbol{Y} \nonumber \\
& = & \int \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{j} } d\boldsymbol{Y} \nonumber \\
& = & \frac{\partial }{\partial \theta_{j} } \int f(\boldsymbol{Y}| \boldsymbol{\theta}) d\boldsymbol{Y}  = \frac{\partial }{\partial \theta_{j} } 1 = 0 \nonumber 
\end{eqnarray}


\end{frame}


\begin{frame}

\begin{scriptsize}
\begin{eqnarray}
\frac{\partial l(\boldsymbol{\theta}| \boldsymbol{Y})  }{\partial \theta_{j} } \frac{\partial l(\boldsymbol{\theta}| \boldsymbol{Y})  }{\partial \theta_{k} }  & = & \frac{1}{f(\boldsymbol{Y} | \boldsymbol{\theta})} \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{j} }\frac{1}{f(\boldsymbol{Y} | \boldsymbol{\theta})} \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{k} } \nonumber \\
E\left[\frac{\partial l(\boldsymbol{\theta}| \boldsymbol{Y})  }{\partial \theta_{j} } \frac{\partial l(\boldsymbol{\theta}| \boldsymbol{Y})  }{\partial \theta_{k} } \right] & = & \int \frac{1}{\left(f(\boldsymbol{Y}| \boldsymbol{\theta}) \right)^2} \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{k} }  \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{j} } f(\boldsymbol{Y} | \boldsymbol{\theta} ) d\boldsymbol{Y} \nonumber \\
-E\left[ \frac{\partial^2 l(\boldsymbol{\theta}| \boldsymbol{Y}) }{\partial \theta_{j} \partial \theta_{k}  } \right] & = & -\int \frac{\frac{\partial^2 f(\boldsymbol{Y}| \boldsymbol{\theta} ) }{\partial \theta_{j} \partial \theta_{k}  } f(\boldsymbol{Y}| \boldsymbol{\theta} ) - \frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{j} }\frac{\partial f(\boldsymbol{Y} | \boldsymbol{\theta}) }{ \partial \theta_{k} }}{f(\boldsymbol{Y} | \boldsymbol{\theta} )^2} f(\boldsymbol{Y} | \boldsymbol{\theta}) d\boldsymbol{Y} \nonumber 
\end{eqnarray}
\end{scriptsize}
\end{frame}



\begin{frame}
\frametitle{Model Based Inference}

\begin{itemize}
\item[1)] Likelihood inference
\item[2)] Logit/Probit
\begin{itemize}
\item[a)] DGP
\item[b)] Optimization
\item[c)] Quantities of Interest
\item[d)] \alert{Perfect + Near Perfect Separation}
\end{itemize}
\alert{\item[3)] Ordered Probit }
\begin{itemize}
\item[a)] DGP
\item[b)] Optimization
\item[c)] Quantities of Interest
\end{itemize}
\end{itemize}


\end{frame}





\begin{frame}
\frametitle{Civil Conflict and Political Instability (h/t: Yamamoto)}
\small

\begin{columns}
\column{.5\textwidth}

\vspace{.5\baselineskip}

Fearon \& Laitin (2003):
	{\footnotesize
	\begin{itemize}
	\item $Y_i$: Civil conflict
	\item $T_i$: Political instability
	\item $W_i$: Geography (log \% mountainous)
	\end{itemize}
	}
\medskip
\onslide<2->{Estimated model:
{\footnotesize
\begin{eqnarray*}
 & & \widehat{\Pr(Y_i=1\mid T_i,W_i)} \\
 & & = \ \text{logit}^{-1}\left(-2.84 + 0.91T_i + 0.35W_i \right)
\end{eqnarray*}
}
}

\medskip
\vspace{-\baselineskip}
\onslide<3->{Predicted probability:
{\footnotesize
$$ \hat\pi(T_i=1,W_i=3.10) \ = \ 0.299 $$
}
}

\medskip
\vspace{-\baselineskip}
\onslide<4->{ATE:
\vspace{-.25\baselineskip}
{\footnotesize
\begin{eqnarray*}
& & \hat\tau \ = \ \frac{1}{n}\sum_{i=1}^n \left\{\hat\pi(1,W_i) - \hat\pi(0,W_i)\right\} \\
& & \quad = \ 0.127
\end{eqnarray*}
}
}

\column{.5\textwidth}
\vspace{-2\baselineskip}
\begin{center}
	\only<2|handout:0>{\includegraphics[height=.85\textheight]{LogitQoIFearonLaitin.pdf}}%
	\only<3|handout:0>{\includegraphics[height=.85\textheight]{LogitQoIFearonLaitin-pi.pdf}}%
	\only<4|handout:0>{\includegraphics[height=.85\textheight]{LogitQoIFearonLaitin-ate.pdf}}%
	\only<5-|handout:1>{\includegraphics[height=.85\textheight]{LogitQoIFearonLaitin-all.pdf}}
\end{center}

\end{columns}
\end{frame}





\begin{frame}
\frametitle{Final Passage Vote for the Affordable Care Act}

\begin{itemize}
\item[] $Y_{i} = $ Vote on ACA 
\item[] $X_{i1} = $ Democrat
\item[] $X_{i2} = $ DW-Nominate Score
\end{itemize}

\pause 

\invisible<1>{\begin{tabular}{ccc}
\hline \hline
	       & Nay & Yea \\
\hline	       
\alert{Republican} &  \alert{178}	   & 	\alert{0}	\\
Democrat   &  34   & 	219	\\
\hline
\end{tabular}}\pause 

\invisible<1-2>{\begin{eqnarray}
\text{Pr}(Y_{i} = 1| X_{i1} = 0 ) & = & 0 \nonumber 
\end{eqnarray}}



\end{frame}

\begin{frame}
\frametitle{Final Passage Vote for the Affordable Care Act}



\begin{footnotesize}
\begin{semiverbatim}
> test\_model<- glm(vote$\sim$dem + ideo, 

data = as.data.frame(final\_votes), family = binomial('logit'))

Warning message:

glm.fit: fitted probabilities numerically 0 or 1 occurred 

\end{semiverbatim}
\end{footnotesize}

\pause 

\invisible<1>{\begin{center}
\begin{tabular} {cc}
\hline \hline
 		  & ACA Vote \\
\hline 		  
Intercept & \alert{ -14.00	}     \\
		  &  \alert{(1670.439)	}	 \\
Democrat  &  \alert{11.67}		\\
		  &  \alert{(1670.439)	}	\\
Ideology  &  -16.86		\\
		  &   (2.71)\\		
\hline \hline		  
\end{tabular}
\end{center}}

\pause 

\begin{eqnarray}
\invisible<1-2>{\widehat{E[Y_{i}|X_{i} = (1, 0, 0.7), \boldsymbol{\beta}^{*}]} & = & 0 \nonumber \\}\pause 
\invisible<1-3>{CI_{95} & = & [0, 1] \text{ Coefficient Simulation } \nonumber\\} \pause 
\invisible<1-4>{CI_{95} & = & [1e-13, 2.3e-10]  \text{ Bootstrap }\nonumber } \pause 
\end{eqnarray}

\invisible<1-5>{We have problems!} 


\end{frame}




\begin{frame}
\frametitle{Is There Anything the Tea Party Doesn't Screw Up?}
\pause 
\begin{footnotesize}
\begin{eqnarray}
\invisible<1>{L(\boldsymbol{\beta}| \boldsymbol{X}, \boldsymbol{Y}) & = & f(\boldsymbol{Y} | \boldsymbol{X}, \boldsymbol{\beta} ) \nonumber \\} \pause 
\invisible<1-2>{& = & \sum_{i=1}^{N} Y_{i} \log \left( \frac{1}{1 + \exp(-\boldsymbol{X}^{'}\boldsymbol{\beta})} \right ) + \alert{(1- Y_{i}) \log  \left( 1  - \frac{1}{1 + \exp(-\boldsymbol{X}^{'}\boldsymbol{\beta})} \right ) } \nonumber } \pause 
\end{eqnarray}
\end{footnotesize}

\invisible<1-3>{Remember: } \pause 
\begin{eqnarray}
\invisible<1-4>{\text{Pr}(Y_{i} = 1| X_{i1} = 0 ) & = & 0 \nonumber } \pause 
\end{eqnarray}


\invisible<1-5>{To fit data: set $\beta_{0} \rightarrow -\infty$ .  } 

\end{frame}


\begin{frame}
\frametitle{Perfect and Near-Perfect Separation}

\pause 

\invisible<1>{\alert{Perfect separation}: one covariate perfectly separates 0's and 1's\\} \pause 
\invisible<1-2>{\alert{Near perfect separation}: one covariate perfectly separates 0's or 1's\\} \pause 

\invisible<1-3>{Solution?: } \pause \\
\invisible<1-4>{\alert{You need to make more assumptions}} 


\end{frame}


\begin{frame}
\frametitle{Add a Few Observations...}

\only<1>{\begin{tabular}{ccc}
\hline \hline
	       & Nay & Yea \\
\hline	       
\alert{Republican} &  \alert{178}	   & 	\alert{0}	\\
Democrat   &  34   & 	219	\\
\hline
\end{tabular}}

\only<2>{\begin{tabular}{ccc}
\hline \hline
	       & Nay & Yea \\
\hline	       
\alert{Republican} &  \alert{178.5}	   & 	\alert{0.5}	\\
Democrat   &  34.5   & 	219.5	\\
\hline
\end{tabular}}


\end{frame}



\begin{frame}
\frametitle{Penalized (Prior)-Logistic Regression}

\begin{itemize}
\item[-] Separation: causes coefficients to diverge 
\item[-] Penalty (prior): force coefficients towards zero 
\end{itemize}

Step 1: Standardize inputs (Gelman et al )
\begin{itemize}
\item[-] Binary variables: mean 0, differ by 1.  
\begin{itemize}
\item[-] Democrats: (30\%).  (0.3, -0.7)
\end{itemize}
\item[-] Other variables: mean 0, sd 0.5.
\end{itemize}



\end{frame}


\begin{frame}
\frametitle{Penalized (Prior)-Logistic Regression}

Step 2: Penalize Likelihood \pause \\
\begin{itemize}
\invisible<1>{\item[1)] Firth's Penalty (Zorn 2005):} \pause 
\end{itemize}


\begin{eqnarray}
\invisible<1-2>{L(\boldsymbol{\beta}| \boldsymbol{X}, \boldsymbol{Y} ) & = & \prod_{i=1}^{N} \pi_{i}^{Y_{i}} (1- \pi_{i})^{1-Y_{i}}\alert{|I(\boldsymbol{\beta})|^{1/2}} \nonumber } \pause 
\end{eqnarray}
\invisible<1-3>{where: } \pause 
\begin{eqnarray}
\invisible<1-4>{\pi_{i} & = &  \frac{1}{1 + \exp(-\boldsymbol{X}_{i}^{'}\boldsymbol{\beta})} \nonumber \\
|I(\boldsymbol{\beta})| & = &  \text{ Determinant of Fisher's information at } \boldsymbol{\beta} \nonumber \\
I(\boldsymbol{\beta}) & = & \boldsymbol{X}^{'} \boldsymbol{W} \boldsymbol{X} \nonumber \\
\boldsymbol{W} & = & \begin{pmatrix}
\pi_{1} (1- \pi_{1}) & 0 & \hdots & 0\\
0 					& \pi_{2}(1- \pi_{2})  & \hdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 		& 	0    & \hdots & \pi_{N} (1- \pi_{N}) \\
			  \end{pmatrix} \nonumber }
\end{eqnarray}




\end{frame}


\begin{frame}
\frametitle{Penalized (Prior)-Logistic Regression}

\begin{eqnarray}
L(\boldsymbol{\beta}| \boldsymbol{X}, \boldsymbol{Y} ) & = & \prod_{i=1}^{N} \pi_{i}^{Y_{i}} (1- \pi_{i})^{1-Y_{i}}\alert{|I(\boldsymbol{\beta})|^{1/2}} \nonumber \\
l(\boldsymbol{\beta}| \boldsymbol{X}, \boldsymbol{Y} ) & = & \sum_{i=1}^{N} Y_{i} \log \pi_{i} + (1-Y_{i} ) \log (1- \pi_{i}) + \frac{1}{2}\log(|I(\boldsymbol{\beta})|) \nonumber 
\end{eqnarray}



\end{frame}


\begin{frame}
\frametitle{Penalized (Prior)-Logistic Regression}

\begin{semiverbatim}
jef\_pri<- function(params, X, Y)\{

	\hspace{0.25in}	beta<- params

	\hspace{0.25in}	y.tilde<- X\%*\%beta

	\hspace{0.25in}	y.prob<- plogis(y.tilde)

	\hspace{0.25in}	temp<- matrix(0, nrow = length(Y), ncol=length(Y))

	\hspace{0.25in}	part1<- Y\%*\%log(y.prob) + (1-Y)\%*\%log(1- y.prob)

	\hspace{0.25in}	diag(temp)<- y.prob*(1-y.prob)

	\hspace{0.25in}	part2<- 0.5*log(det(t(X)\%*\%temp\%*\%X))

	\hspace{0.25in}	out<- part1 + part2

	\}

firth<- optim(rnorm(3), jef\_pri, method = 'BFGS', 

control=list(fnscale=-1), hessian=T, 

X = cbind(1, dem, ideo), Y =clean[,3] )	


\end{semiverbatim}


\end{frame}

\begin{frame}
\frametitle{Comparison}



\begin{center}
\begin{tabular} {ccc}
\hline \hline
 		  & ACA Vote (GLM) & Firth  \\
\hline 		  
Intercept & -14.00	   &   -5.70\\
		  &  (1670.439)	&	(24.68) \\
Democrat  &  11.67		&  3.30\\
		  &  (1670.439)	&	(42.10)\\
Ideology  &  -16.86		& -17.72\\
		  &   (2.71)    &  (2.85) \\		
\hline \hline		  
\end{tabular}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Penalized (Prior) Logistic Regression}

\pause 

\invisible<1>{Alternative Prior (Gelman et al 2005): } \pause 
\begin{itemize}
\invisible<1-2>{\item[-] Add 1/2 to success and failure: $t$ distribution, 7 degrees of freedom and \alert{scale} = 2.5} \pause 
\invisible<1-3>{\item[-] Suggestion \text{Cauchy} (DOF = 1) with scale 2.5. } \pause 
\begin{itemize}
\invisible<1-4>{\item[-] Prioritize $\beta<0.5$} \pause 
\invisible<1-5>{\item[-] Occasionally allows very large values (Cauchy)} \pause 
\end{itemize}
\end{itemize}

\invisible<1-5>{\begin{semiverbatim}

lst<- function(x, nu, mu, sigma2)\{
	
\hspace{0.3in}	part1<- lgamma( (nu + 1)/2)

\hspace{0.3in}	part2<- lgamma(nu/2)

\hspace{0.3in}	part3<- sqrt(pi *nu*sqrt(sigma2))

\hspace{0.3in}	part4<- 1 + (1/nu)*(( (x- mu)\^2)/sigma2)

\hspace{0.3in}	part4<- ( - (nu + 1)/2)*log(part4)
	
\hspace{0.3in}	out<- part4

\hspace{0.3in}	return(out)	

\}


\end{semiverbatim}}


\end{frame}


\begin{frame}
\pause 

\begin{semiverbatim}

\invisible<1>{log\_t<- function(params, X, Y, nu, mu, sigma2)\{

	beta<- params

	prior<- 0

	for(k in 2:ncol(X))\{

		prior<- prior + lst(beta[k], nu, mu, sigma2)
	
		\}

	prior<- prior + lst(beta[1], 1, 0, 10)
	
	y.tilde<- X\%*\%beta

	y.prob<- plogis(y.tilde)

	out<- Y\%*\%log(y.prob) + (1- Y)\%*\%log(1- y.prob)

	out<- out + prior

	return(out)
		
	\}
} \pause 

\invisible<1-2>{cauch<- optim(rnorm(3), log\_t, method='BFGS', 

control=list(fnscale=-1),  hessian=T, 

X= cbind(1, dem, ideo), Y = clean[,3], nu = 1,

 mu = 0, sigma2=2.5)
}\pause 
\end{semiverbatim}

\invisible<1-3>{In practice: {\tt bayesglm} in {\tt library(arm)} is awesome!} 


\end{frame}

\begin{frame}
\frametitle{Comparison}

\begin{center}
\begin{tabular} {cccc}
\hline \hline
 		  & ACA Vote (GLM) & Firth  & Cauchy \\
\hline 		  
Intercept & -14.00	   &   -5.70 & -3.23\\
		  &  (1670.439)	&	(24.68) & (0.99)\\
Democrat  &  11.67		&  3.30  & -0.19\\
		  &  (1670.439)	&	(42.10) & (1.12) \\
Ideology  &  -16.86		& -17.72   & -16.25\\
		  &   (2.71)    &  (2.85) & (2.65)\\		
\hline \hline		  
\end{tabular}
\end{center}

\end{frame}


\begin{frame}

\huge 
Ordered Outcome Data

\end{frame}



\begin{frame}
\frametitle{Modeling Ordered Outcomes (ht: Yamamoto)}
\begin{itemize}
    \item Suppose that the $J$ choices are \alert{ordered} in a substantively meaningful way
\pause
    \item Examples:
        \begin{itemize}
            \item ``Likert scale'' in survey questions (``strongly agree'', ``agree'', etc.)
            \item Party positions (extreme left, center left, center, right, extreme right)
            \item Levels of democracy (autocracy, anocracy, democracy)
            \item Health status (healthy, sick, dying, dead)
        \end{itemize}
    \medskip
\pause
    \item Why not use continuous outcome models? \\
\pause
        $\longrightarrow$ Don't want to assume equal distances between levels
\pause
    \item Why not use categorical outcome models? (on Monday) \\
\pause
        $\longrightarrow$ Don't want to waste information about ordering

\end{itemize}

\end{frame}

%%%%%%%%% frame %%%%%%%%%
\begin{frame}
\frametitle{Ordered Logit and Probit Models  (ht: Yamamoto)}
\begin{itemize}
    \item Again, the \alert{latent variable} representation: 
        $ Y_i^\ast \ = \ X_i^{'} \beta + \epsilon_{i} $
\pause
    \item Assume that $Y_i^\ast$ gives rise to $Y_i$ based on the following scheme:
    {\small
        $$Y_i \ = \ \left\{\begin{array}{ccccccc} 
      1 & \text{if} & -\infty (= \psi_0)  & < & Y_i^\ast & \le & \psi_1, \\
      2 & \text{if} & \psi_1 & < & Y_i^\ast & \le & \psi_2, \\
      \vdots & & & & \vdots & & \\
      J & \text{if} & \psi_{J-1} & < & Y_i^\ast & \le & \infty (= \psi_J) 
        \end{array}\right. $$
    }
where $\psi_1, ..., \psi_{J-1}$ are the \alert{threshold parameters} to be estimated
\pause
    \item If $X_i$ contains an intercept, one of the $\psi$'s must be fixed for identifiability
    (typically $\psi_1 = 0$)
\medskip
\pause
    \item $\epsilon_{j} \sim_{\text{iid}}$ logistic $\Rightarrow$ the \alert{ordered logit} model:
    {\small
        $$ \Pr(Y_i \leq j \mid X_i) \ = \ \frac{\exp(\psi_j - X_i^{'}\beta)}
            {1 + \exp(\psi_j - X_i^\top\beta)} $$
    }
    \item $\epsilon_{j} \sim_{\text{iid}} \text{Normal}(0,1)$ $\Rightarrow$ the \alert{ordered probit} model:
    {\small
        $$ \Pr(Y_i \leq j \mid X_i) \ = \ \Phi\left(\psi_j - X_i^{'}\beta\right) $$
    }
\end{itemize}
\end{frame}

%%%%%%%%% frame %%%%%%%%%
\begin{frame}
\frametitle{Ordered Logit and Probit Models (ht: Yamamoto)}
\begin{center}
\only<1>{\includegraphics[height=.9\textheight]{ordered1.pdf}}%
\only<2>{\includegraphics[height=.9\textheight]{ordered2.pdf}}%
\only<3>{\includegraphics[height=.9\textheight]{ordered3.pdf}}%
\only<4>{\includegraphics[height=.9\textheight]{ordered4.pdf}}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Ordered Logit and Probit}

\pause 
\begin{eqnarray}
\invisible<1>{P(Y_{i}=J) & = & \int_{\psi_{j-1}}^{\psi_{j} } \phi(\tilde{y}| \boldsymbol{X}_{i}^{'}\boldsymbol{\beta}) d\tilde{y} \nonumber \\
		    & = & \Phi(\psi_{j} | \boldsymbol{X}_{i}^{'}\boldsymbol{\beta}) - \Phi(\psi_{j-1} | \boldsymbol{X}_{i}^{'}\boldsymbol{\beta}) \nonumber }\pause 
\end{eqnarray}		    
\invisible<1-2>{Implies a likelihood of:} \pause 

\begin{eqnarray}
\invisible<1-3>{L(\boldsymbol{\beta}, \boldsymbol{\Psi} | \boldsymbol{X}, \boldsymbol{Y}) & = & \prod_{i=1}^{N}\left[\prod_{j=1}^{J}[ \Phi(\psi_{j} | \boldsymbol{X}_{i}^{'}\boldsymbol{\beta}) - \Phi(\psi_{j-1} | \boldsymbol{X}_{i}^{'}\boldsymbol{\beta})  ]^{I(Y_{i} = j)}    \right] \nonumber \\}\pause 
\invisible<1-4>{l(\boldsymbol{\beta}, \boldsymbol{\Psi} | \boldsymbol{X}, \boldsymbol{Y}) & = & \sum_{i=1}^{N} \left(\sum_{j=1}^{J} I(Y_{i} =j)\log\left[ \Phi(\psi_{j} | \boldsymbol{X}_{i}^{'}\boldsymbol{\beta}) - \Phi(\psi_{j-1} | \boldsymbol{X}_{i}^{'}\boldsymbol{\beta}) \right]   \right) \nonumber } \pause 
\end{eqnarray}	
\invisible<1-5>{fit with {\tt polr} package} 


\end{frame}







%%%%%%%%% frame %%%%%%%%%
\begin{frame}
\frametitle{Calculating Quantities of Interest  (ht: Yamamoto)}
{\small
\vspace{-.05in}
\begin{itemize}
    \item Predicted probability:
    \begin{eqnarray*}
    \pi_{ij}(X_i) & \equiv & \Pr(Y_i = j\mid X_i) \ = \
        \Pr(Y_i \leq j \mid X_i) - \Pr(Y_i \leq j-1 \mid X_i) \\
    & = & \left\{ \begin{array}{ll}
        \frac{\exp(\psi_j - X_i^{'}\beta)}{1 + \exp(\psi_j - X_i^{'}\beta)}
        - \frac{\exp(\psi_{j-1} - X_i^{'}\beta)}{1 + \exp(\psi_{j-1} - X_i^{'}\beta)}
        & \text{for logit} \\
        \Phi\left(\psi_j - X_i^{'}\beta\right) - \Phi\left(\psi_{j-1} - X_i^{'}\beta\right)
        & \text{for probit} \end{array} \right.
    \end{eqnarray*}
\pause
    \item ATE (APE): \ $\tau_j \  = \ \text{E}\left[\pi_{j}(T_i=1, W_i) - \pi_{j}(T_i=0, W_i)\right]$
\pause
\medskip
    \item Estimate $\beta$ and $\psi$ via MLE, plug the estimates in, replace $\text{E}$ with 
    $\frac{1}{n}\sum$, and compute CI by delta or MC or bootstrap
\medskip
\pause
    \item Note that $X_i^{'}\beta$ appears both before and after the minus sign in $\pi_{ij}$ \\
\pause
    $\longrightarrow$ Direction of effect of $X_i$ on $Y_{ij}$ is ambiguous (except top and bottom)
\pause
    $\longrightarrow$ Again, \alert{calculate quantities of interest, not just coefficients}
% \medskip
%     \item Possible generalization: Allow $\beta$ to vary across levels by setting $\beta_j$
%     \item Relaxes the \alert{proportional odds} or \alert{parallel regression} assumption
%     \item Easy to estimate, but the latent variable interpretation breaks down
\end{itemize}
}
\end{frame}

%%%%%%%%% frame %%%%%%%%%
\begin{frame}[fragile]
\frametitle{Immigration and Media Priming(ht: Yamamoto)}
\small
Brader, Valentino and Suhay (2008):
\begin{itemize}
   \item $Y_i$: Ordinary response to question about increasing immigration
   \item $T_{1i},T_{2i}$: Media cues (immigrant ethnicity $\times$ story tone)
   \item $W_i$: Respondent age and income
\end{itemize}
\pause
\medskip
\begin{columns}[t]
\column{.4\textwidth}
Estimated coefficients:

\begin{scriptsize}
\begin{verbatim}
Coefficients:
         Value  s.e.   t 
tone      0.27  0.32  0.85
eth      -0.33  0.32 -1.02
ppage     0.01  0.02  1.40
ppincimp  0.00  0.03  0.06
tone:eth  0.90  0.46  2.16

Intercepts:
    Value  s.e.    t 
1|2 -1.93  0.58  -3.32
2|3 -0.12  0.55  -0.21
3|4  1.12  0.56   2.01
\end{verbatim}
\end{scriptsize}
\pause
\column{.5\textwidth}
ATE:
\vspace{-1.25\baselineskip}
\begin{center}
\includegraphics[width=.95\textwidth]{BraderATE.pdf}
\end{center}

\end{columns}

\end{frame}



\begin{frame}
\frametitle{Model Based Inference}

\begin{itemize}
\item[1)] Likelihood inference
\item[2)] Logit/Probit
\begin{itemize}
\item[a)] DGP
\item[b)] Optimization
\item[c)] Quantities of Interest
\item[d)] \alert{Perfect + Near Perfect Separation}
\end{itemize}
\alert{\item[3)] Ordered Probit }
\begin{itemize}
\item[a)] DGP
\item[b)] Optimization
\item[c)] Quantities of Interest
\end{itemize}
\item[4)] Choice Models: Multinomial Logit/Probit
\end{itemize}




\end{frame}




\end{document} 