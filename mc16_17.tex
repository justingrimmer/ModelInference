\documentclass{beamer}

%\usepackage[table]{xcolor}
\mode<presentation> {
  \usetheme{Boadilla}
%  \usetheme{Pittsburgh}
%\usefonttheme[2]{sans}
\renewcommand{\familydefault}{cmss}
%\usepackage{lmodern}
%\usepackage[T1]{fontenc}
%\usepackage{palatino}
%\usepackage{cmbright}
  \setbeamercovered{transparent}
\useinnertheme{rectangles}
}
%\usepackage{normalem}{ulem}
%\usepackage{colortbl, textcomp}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{structure}{fg= black}
\definecolor{trial}{cmyk}{1,0,0, 0}
\definecolor{trial2}{cmyk}{0.00,0,1, 0}
\definecolor{darkgreen}{rgb}{0,.4, 0.1}
\newtheorem{com}{Comment}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
 \numberwithin{equation}{section}


\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif}

\beamertemplatesolidbackgroundcolor{white}  \setbeamercolor{alerted
text}{fg=red}
\usepackage{multirow}

\setbeamertemplate{caption}[numbered]\newcounter{mylastframe}

%\begin{beamerboxesrounded}[upper=uppercol,lower=lowercol,shadow=true]{Block}
%$A = B$.
%\end{beamerboxesrounded}}
\renewcommand{\familydefault}{cmss}
\usepackage[all]{xy}
\usepackage{tikz}
\usetikzlibrary{arrows}
%\usepackage{palatino}
%\usepackage{eulervm}
\usecolortheme{lily}
%\usepackage[latin1]{inputenc}

\title[Methodology III] % (optional, nur bei langen Titeln n√∂tig)
{Political Methodology III: Model Based Inference}

\author{Justin Grimmer}
\institute[Stanford University]{Associate Professor\\Department of Political Science \\  Stanford University}
\vspace{0.3in}

\date{May 23rd, 2017}

\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}

\begin{itemize}
\item[1)] Task:
\begin{itemize}
\item[-] Estimate conditional expectation function (regression) $E[Y|\boldsymbol{X}]$
\item[-] Estimate empirical distribution function $\hat{f}(x)$
\item[-] Estimate conditional empirical distribution function $\hat{f}(x| \boldsymbol{Z})$
\end{itemize}
\item[2)] Objective Function
\begin{itemize}
\item[-] Mean Square Error $\leadsto$ Average (Predictive) Risk
\item[-] Balance Bias-Variance Tradeoff
\end{itemize}
\item[3)] Optimization
\begin{itemize}
\item[-] Local Linear Regression
\end{itemize}
\item[4)] Validation
\begin{itemize}
\item[-] Bias/Variance Tradeoff in fitting data 
\item[-] LOOCV will be a workhorse tool
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\huge 

Nonparametric Regression

\end{frame}



\begin{frame}
\frametitle{Motivating Example: Birth Weight}

\pause 
\begin{itemize}
\invisible<1>{\item[-] \alert{Birth Weight} is associated with a wide array of developmental outcomes}\pause 
\invisible<1-2>{\item[-] \alert{Birth Weight} varies significantly across socioeconomic and racial groups}\pause 
\invisible<1-3>{\item[-] Understand how Birth Weight varies with a variety of attributes}\pause 
\end{itemize}

\begin{itemize}
\invisible<1-4>{\item[-] What are differences in birth weight across racial groups?  }\pause 
\invisible<1-5>{\item[-] Are differences larger (smaller) within sub-groups (smokers, obese, etc)?} \pause 
\end{itemize}

\invisible<1-6>{Suppose that we observe birth weight for a set of $N$ observations of random variable birth weight, } \pause 
\begin{eqnarray}
\invisible<1-7>{\boldsymbol{y} & = & (y_{1}, y_{2}, \hdots, y_{N}) \nonumber } \pause 
\end{eqnarray}

\invisible<1-8>{We'll also observe a set of $J$ \alert{covariates} for each observation $i$} \pause 
\begin{eqnarray}
\invisible<1-9>{\boldsymbol{x}_{i} & = & (x_{i1}, x_{i2}, \hdots, x_{iJ} ) \nonumber }
\end{eqnarray}


\end{frame}

\begin{frame}
\frametitle{Conditional Expectation Functions and Regressions}


\begin{defn}
For any two random variables $Y$ and $X$ define the regression of $Y$ on $X$ as 
\begin{eqnarray}
f(x) & \equiv & E[Y| X= x] = \int_{-\infty}^{\infty} y f(y|X=x) dy  \nonumber 
\end{eqnarray}
\end{defn}
\pause 


\invisible<1>{\begin{itemize}
\item[-] \alert{Linear Regression} is a special case with:
\begin{eqnarray}
f(x) & = & \alpha + \beta x \nonumber 
\end{eqnarray}
\item[-] $E[Y|X]$ is called a \alert{conditional expectation function}
\end{itemize}
}
\pause 

\invisible<1-2>{We're going to consider more flexible functional forms for $f(x)$} \pause \\
\invisible<1-3>{Our estimator $\hat{f}(x)$ will estimate $f(x)$}


\end{frame}



\begin{frame}
\frametitle{Estimating $E[Y|X=x]$}

\alert{Easy case}: suppose $X$ is discrete\pause   \\
\invisible<1>{Further suppose that $N \rightarrow \infty$.  Infinite number of realizations of $Y$, $(y_{1}, \hdots, y_{N}$) } \pause  \\
\invisible<1-2>{And accompanying covariates $(x_{1}, \hdots, x_{N})$, then } \pause \\
\begin{eqnarray}
\invisible<1-3>{\widehat{f}(x) & = & \text{average in each bin} \nonumber \\
& = & \frac{\sum_{i=1}^{N} y_{i} I(x_{i} = x)}{\sum_{i=1}^{N} I(x_{i} = x) } }   \nonumber
\end{eqnarray}

\end{frame}


\begin{frame}
\frametitle{Bias-Variance Tradeoff} 
\only<1-4>{
\invisible<1>{\alert{Hard Case}: suppose $X$ is continuous }  \\
\invisible<1-2>{Then all values of $x_{1}, x_{2}, \hdots, x_{N}$ are distinct. }   \\
\invisible<1-3>{If we calculate conditional mean naively, we get the observed values of $y_{1}, \hdots, y_{N}$:}  \\
}

\pause \pause \pause \pause 

\only<5>{
\begin{columns}[]
\column{0.6\textwidth}
\scalebox{0.5}{\includegraphics{BR3.pdf}} 

\column{0.4\textwidth} 
$<$ R Code $>$
{\tt library(ggplot2) } \\
{\tt  qplot(lwt, bwt, geom=c('point'), xlab='Mothers Weight', ylab='Birth Weight')    } \\
{\tt $ +  \text{geom}\_{}\text{line}()$} 
\end{columns}
}

\end{frame}

\begin{frame}
\frametitle{Bias-Variance Tradeoff} 

\only<1-4>{Minimize variance: \pause \invisible<1>{assume $\widehat{f}(x)$ is constant } 
\begin{eqnarray}
\invisible<1-2>{\widehat{f}(x) & = &    \frac{\sum_{i=1}^{N} Y_{i} }{N}  \nonumber }  
\end{eqnarray}

\invisible<1-3>{This yields: }  } 

\pause \pause \pause 

\only<5>{
\begin{columns}[] 
\column{0.6\textwidth} 
\scalebox{0.5}{\includegraphics{BR4.pdf}} 
\column{0.4\textwidth} 
{\tt library(ggplot2) } \\
{\tt qplot(lwt, bwt, geom=c('point'), xlab='Mother\'s Weight', ylab='Birth Weight') }\\
{\tt $+ \text{geom}\_\text{abline}$\\
(intercept=mean(bwt), slope=0, lwd=2, col='cornflowerblue') } 
\end{columns} 
}

\end{frame}


\begin{frame}
\huge 

Compromise 

\end{frame}




\begin{frame}
\frametitle{Curse of Dimensionality}

\begin{itemize}
\item We're going to learn around a \alert{neighborhood} of a point \pause 
\invisible<1>{\item Problem: high-dimensional space is sparse} \pause 
\invisible<1-2>{\item Suppose We have $N$ draws of $X \sim \text{Uniform}(-1,1)$} \pause 
\invisible<1-3>{\item How many points are between $[-0.1,0.1]$?} \pause 
\begin{eqnarray}
\invisible<1-4>{N \times \text{Pr}(X \in [-0.1, 0.1]) & = & N \times \frac{0.2}{2} = \frac{N}{10} \nonumber } \pause 
\end{eqnarray} 
\invisible<1-5>{\item Suppose $N$ draws $\boldsymbol{X} = (X_{1}, X_{2}, \hdots, X_{10})$ where } \pause 
\begin{eqnarray}
\invisible<1-6>{X_{i} \sim_{iid} \text{Uniform}(-1, 1) \nonumber } \pause 
\end{eqnarray}
\invisible<1-7>{\item How many points between $-0.1$ and $0.1$ on all dimensions?} \pause 
\begin{eqnarray}
\invisible<1-8>{N \times \text{Pr}(\boldsymbol{X} \in [-0.1, 0.1]\times[-0.1, 0.1]\times \hdots \times [-0.1,0.1]) & = & \nonumber \\} \pause 
\invisible<1-9>{N \times \left(\frac{1}{10}\right)^{10} = \frac{N}{10,000,000,000} && \nonumber }
\end{eqnarray}
\end{itemize}



\end{frame}


\begin{frame}

\huge 

Bias-Variance Tradeoff 

\end{frame}


\begin{frame}
\frametitle{Modeling the Relationship}


\begin{eqnarray}
Y & = & f(x_{i}) + \epsilon_{i} \nonumber 
\end{eqnarray}

Suppose $f(x_{i}) \equiv \beta_{0} + \beta_{j=1}^{J} x_{ij}$ and $\boldsymbol{x} = (1, x_{1}, \hdots, x_{J})$.  \pause 


\begin{small}
\begin{eqnarray}
\invisible<1>{\widehat{\boldsymbol{\beta}} & = & \left(\boldsymbol{X}^{'}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{'}\boldsymbol{Y} \nonumber \\} \pause 
\invisible<1-2>{\hat{f}(\boldsymbol{X}) \equiv \widehat{\boldsymbol{Y}} & = & \underbrace{\boldsymbol{X}\left(\boldsymbol{X}^{'}\boldsymbol{X} \right)^{-1} \boldsymbol{X}^{'}}_{\boldsymbol{H}} \boldsymbol{Y} \nonumber \\} \pause 
\invisible<1-3>{J + 1 & = & \text{tr}(\boldsymbol{H}) \nonumber \\} \pause 
\invisible<1-4>{\hat{f}(\boldsymbol{x}) & = & \boldsymbol{x}^{'} \widehat{\boldsymbol{\beta}} \nonumber \\} \pause 
\invisible<1-5>{& = & \underbrace{\boldsymbol{x}^{'}}_{1 \times J} \underbrace{\left(\boldsymbol{X}^{'}\boldsymbol{X}\right)^{-1}}_{J \times J} \underbrace{\boldsymbol{X}^{'}}_{J \times N} \underbrace{\boldsymbol{Y} }_{N \times 1} \nonumber \\} \pause
 \invisible<1-6>{& = & \sum_{i=1}^{N} h_{i}(\boldsymbol{x}) Y_{i} \nonumber } 
\end{eqnarray}
\end{small}

\end{frame}



\begin{frame}
\frametitle{Modeling the Relationship}

\only<1>{
\begin{defn}
We will say $\hat{f}(\boldsymbol{x})$, an estimator of $f(\boldsymbol{x})$, is a \alert{linear smoother} if for each $\boldsymbol{x}$ there exists a vector $\boldsymbol{h}(\boldsymbol{x}) = (h_{1} (\boldsymbol{x}), h_{2}(\boldsymbol{x}), \hdots, h_{N} (\boldsymbol{x})) $ such that 

\begin{eqnarray}
\hat{f}(\boldsymbol{x}) & = & \sum_{i=1}^{N} h_{i}(\boldsymbol{x})Y_{i} \nonumber 
\end{eqnarray}

This implies that fitted values 
$\widehat{\boldsymbol{Y}} \ = \ (\hat{f}(\boldsymbol{x}_{1}), \hat{f}(\boldsymbol{x}_{2}), \hdots, \hat{f}(\boldsymbol{x}_{N}) ) $ and 

\begin{eqnarray}
\widehat{\boldsymbol{Y}} & = & \boldsymbol{H} \boldsymbol{Y} \nonumber 
\end{eqnarray}

where $i^{\text{th}}$ row is $\boldsymbol{h}(\boldsymbol{x}_{i}) =(h_{1} (\boldsymbol{x}_{i} ), h_{2}(\boldsymbol{x}_{i}), \hdots, h_{N} (\boldsymbol{x}_{i})) $


\end{defn}
}

\only<2>{
	
\begin{defn}
The matrix $\boldsymbol{H}$ is the smoothing matrix. The row $h(\boldsymbol{x}_{i})$ is called the effective kernel for observation $i$, and the effective number of parameters $\nu = \text{tr}(\boldsymbol{H})$.  


\end{defn}


}


\end{frame}


\begin{frame}
\frametitle{``Regressogram" }

\only<1>{
\begin{itemize}
\item Suppose we have one variable $x_{i}$
\item Suppose we divide our data into $K$ bins,$B_{k} = \{x: a<x<b\}$, $k = 1, 2, \hdots, K$ so that there are $3$ observations in each bin $|B_{k}| = 3$ for all $k$
\item We will say that the fitted value for $x_{i}$ is then 
\begin{eqnarray}
\hat{f}(x_{i}) & = &  \sum_{x_{i} \in B_{k}} \frac{Y_{i}}{3} \nonumber 
\end{eqnarray}
\end{itemize}
}
\only<2>{
\begin{itemize}
\item In smoothing terms (sorting observations according to $x_{i}$ values)
\begin{eqnarray}
\boldsymbol{h}(x_{i}) & = & (0, 0,0, \frac{1}{3}, \frac{1}{3}, \frac{1}{3}, 0, 0, 0 ) \nonumber \\
\boldsymbol{H} & = & \begin{pmatrix}
\frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 &0 &  0 & 0 & 0 & 0 \\
 \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 & 0 \\
  \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 & 0 \\
 0           & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 \\
  0           & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 \\
  0           & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 \\
0 	   & 0  	&  0      &  0     & 0 & 0  & \frac{1}{3}& \frac{1}{3} & \frac{1}{3} \\
0 	   & 0  	&  0      &  0     & 0 & 0  & \frac{1}{3}& \frac{1}{3} & \frac{1}{3} \\
0 	   & 0  	&  0      &  0     & 0 & 0  & \frac{1}{3}& \frac{1}{3} & \frac{1}{3} \\
\end{pmatrix} \nonumber 
\end{eqnarray}
\end{itemize}
}

\end{frame}



\begin{frame}
\frametitle{Local Averages}

\begin{itemize}
\item Define $B_{x}  =\{i: |x_{i} - x|< b  \}$ and $|B_{x}| = n_{x}$. 
\item If $n_{x}>0$,
\begin{eqnarray}
\hat{f}(x) & = & \sum_{i \in B_{x} } \frac{Y_{i}}{n_{x}} \nonumber 
\end{eqnarray}
\end{itemize}
In smoothing terms
\begin{eqnarray}
h_{i} (x)  & = & 0 \text{ if } |x_{i} - x| >b \nonumber \\
h_{i} (x) & = & \frac{1}{n_{x}} \text{ if }  |x_{i} - x| <b  \nonumber
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{More Sophisticated Weights $\leadsto$ Kernels}

\begin{itemize}
\item Previous examples are binary (in/out) \pause 
\invisible<1>{\item Continuous weights via \alert{Kernel}} \pause 
\invisible<1-2>{\item Some famous examples (Define $I(x) \ \equiv \ 1 \text{ if } |x|<1, \text{ otherwise } 0
$)
\begin{eqnarray}
K(x) & = & \frac{1}{2}I(x) \text{ (Box Car) } \nonumber \\
K(x) & = & \frac{1}{\sqrt{2\pi}} \exp\left(\frac{-x^2}{2}  \right) \text{ (Gaussian) } \nonumber \\
K(x) &= & \frac{3}{4} (1- x^2) I(x) \text{ (Epanechnikov) } \nonumber \\
K(x) & = & \frac{70}{81} ( 1- |x|^3)^3I(x) \text{ (Tri-cube) } \nonumber 
\end{eqnarray}
}

\end{itemize}



\end{frame}


\begin{frame}

\begin{center}
\scalebox{0.6}{\includegraphics{Kernels1.pdf}}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Kernels and Bandwidth}

Define $b>0$ to be the \alert{bandwidth}. \pause  

\invisible<1>{\begin{eqnarray}
K\left(\frac{x - x_{i}}{b} \right) & = & \text{ Scales difference between $x$ and $x_{i}$ by $b$ } \nonumber 
\end{eqnarray}} \pause 
\invisible<1-2>{Consider the box-car kernel} \pause 

\begin{eqnarray}
\invisible<1-3>{\frac{1}{2}I\left(\frac{x - x_{i}}{1} \right) & = & \text{ $\frac{1}{2}$ if $| x - x_{i}| < 1$ } \nonumber \\} \pause 
\invisible<1-4>{\frac{1}{2}I\left(\frac{x - x_{i}}{100} \right) & = & \text{ $\frac{1}{2}$ if $| x - x_{i}| < 100$ } \nonumber \\} \pause 
\invisible<1-5>{\frac{1}{2}I\left(\frac{x - x_{i}}{0.01} \right) & = & \text{ $\frac{1}{2}$ if $| x - x_{i}| < 0.01$ } \nonumber } \pause 
\end{eqnarray}

\invisible<1-6>{$b \leadsto $ controls the sensitivity of Kernel } 


\end{frame}


\begin{frame}
\frametitle{Local Regression}

\begin{defn}
Suppose $b>0$. Define the local regression estimator $\hat{f}(x)$ of $f(x)$ as 

\begin{eqnarray}
\hat{f}(x) & = & \sum_{i=1}^{N} h_{i}(x) Y_{i} \nonumber 
\end{eqnarray}

where
\begin{eqnarray}
h_{i}(x) & = & \frac{K\left( \frac{x - x_{i}}{b} \right) }{\sum_{j=1}^{N} K\left(\frac{x - x_{j} }{b} \right)   } \nonumber 
\end{eqnarray}


$\boldsymbol{H}  = N \times N $ matrix with $H_{ij} = h_{j}(x_{i})$ 

\end{defn}


\end{frame}



\begin{frame}


\begin{columns}

\column{0.5\textwidth}
\only<1>{\scalebox{0.45}{\includegraphics{D0Smooth1.pdf}}}
\only<2>{\scalebox{0.45}{\includegraphics{D0Smooth2.pdf}}}
\only<3>{\scalebox{0.45}{\includegraphics{D0Smooth3.pdf}}}
\only<4>{\scalebox{0.45}{\includegraphics{D0Smooth4.pdf}}}


\column{0.4\textwidth}
\only<1>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 0, span = 0.1) } }
\only<2>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 0, span = 0.4) } }
\only<3>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 0, span = 0.7) } }
\only<4>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 0, span = 1) } }

\end{columns}


\end{frame}



\begin{frame}
\frametitle{Local Regression}

\begin{eqnarray}
w_{i}(x) & = & K\left(\frac{x_{i} - x}{b} \right) \nonumber \\
\hat{a} & = & \text{arg min}_{a} \sum_{i=1}^{N} w_{i}(x) (Y_{i} - a )^2 \nonumber \\
\hat{a} & = & \frac{ \sum_{i=1}^{N} w_{i}(x)Y_{i}}{ \sum_{i=1}^{N} w_{i}(x) } \nonumber \\
\hat{a} & = & \hat{f}(x) \nonumber 
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{Local Polynomial regression}

\begin{eqnarray}
(\hat{a}, \hat{b}, \hat{c} ) & = & \text{arg min}_{a,b,c} \sum_{i=1}^{N} w_{i}(x)(Y_{i} - a - bx_{i} - c x_{i}^2)^2 \nonumber 
\end{eqnarray}
Define $\boldsymbol{X}_{x}$ to be an $N \times 3$ matrix:
\begin{eqnarray}
\boldsymbol{X}_{x} & = & \begin{pmatrix}
1 & x_{1} - x & (x_{1} - x)^2 \\
1 & x_{2} - x & (x_{2} - x)^2 \\
\vdots & \vdots & \vdots \\
1 & x_{N} - x & (x_{N} - x)^2 \\
\end{pmatrix} \nonumber 
\end{eqnarray}
$\boldsymbol{W}_{x} = N \times N $ matrix where $W_{ii} = w_{i}(x)$.  

\begin{eqnarray}
\hat{f}(x) & = & \left( \boldsymbol{X}_{x}^{'}\boldsymbol{W}_{x}\boldsymbol{X}_{x}   \right)^{-1} \boldsymbol{X}_{x}^{'}\boldsymbol{W}_{x} \boldsymbol{Y} \nonumber 
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{Local Polynomial Regression}

\begin{defn}
Suppose $b>0$.  Define the local polynomial regression estimator $\hat{f}(x)$ of $f(x)$ as 

\begin{eqnarray}
\hat{f}(x) & = & \left( \boldsymbol{X}_{x}^{'}\boldsymbol{W}_{x}\boldsymbol{X}_{x}   \right)^{-1} \boldsymbol{X}_{x}^{'}\boldsymbol{W}_{x} \boldsymbol{Y} \nonumber 
\end{eqnarray}

We can write this as $\hat{f}(x) = \sum_{i=1}^{N} h_{i}(x) Y_{i}$, where $\boldsymbol{h}(x) = (h_{1}(x), h_{2}(x), \hdots, h_{N} (x))$

\begin{eqnarray}
h(x)  & = & \boldsymbol{e}_{1} \left( \boldsymbol{X}_{x}^{'}\boldsymbol{W}_{x}\boldsymbol{X}_{x}   \right)^{-1} \boldsymbol{X}_{x}^{'}\boldsymbol{W}_{x}  \nonumber 
\end{eqnarray}

where $\boldsymbol{e}_{1} = (1, 0, \hdots, 0)$.  \\

$\boldsymbol{H}$ is an $N \times N$ matrix with typical entry $H_{ij} = h_{j}(x_{i})$. 

\end{defn}
\end{frame}



\begin{frame}


\begin{columns}

\column{0.5\textwidth}
\only<1>{\scalebox{0.45}{\includegraphics{D0Smooth5.pdf}}}
\only<2>{\scalebox{0.45}{\includegraphics{D0Smooth6.pdf}}}
\only<3>{\scalebox{0.45}{\includegraphics{D0Smooth7.pdf}}}
\only<4>{\scalebox{0.45}{\includegraphics{D0Smooth8.pdf}}}
\only<5>{\scalebox{0.45}{\includegraphics{D0Smooth9.pdf}}}
\only<6>{\scalebox{0.45}{\includegraphics{D0Smooth10.pdf}}}
\only<7>{\scalebox{0.45}{\includegraphics{D0Smooth11.pdf}}}
\only<8>{\scalebox{0.45}{\includegraphics{D0Smooth12.pdf}}}
\only<9>{\scalebox{0.45}{\includegraphics{D0Smooth4.pdf}}}
\only<10>{\scalebox{0.45}{\includegraphics{D0Smooth8.pdf}}}
\only<11>{\scalebox{0.45}{\includegraphics{D0Smooth12.pdf}}}



\column{0.4\textwidth}

\only<1>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 0.1) 
}}

\only<2>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 0.4) 
}}

\only<3>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 0.7) 
}}

\only<4>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 1) 
}}


\only<5>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 2, span = 0.1) 
}}

\only<6>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 2, span = 0.4) 
}}

\only<7>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 2, span = 0.7) 
}}

\only<8>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 2, span = 1) 
}}

\only<9>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 0, span = 1) } }

\only<10>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 1) 
}}

\only<11>{{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 1) 
}}
\end{columns}

\end{frame}



\begin{frame}

\huge 

Bandwidth Selection


\end{frame}

\begin{frame}
\frametitle{Leave One Out Cross Validation LOOCV}

\begin{itemize}
\item Define $\hat{f}_{-i,b}(x)$ to be the linear smooth estimtator without observation $i$ and bandwidth $b$.  
\item The LOOCV statistic will be 

\begin{eqnarray}
\text{CV(b)} & = & \frac{1}{N} \sum_{i=1}^{N}\left(Y_{i} - \hat{f}_{-i,b}(x)  \right)^2 \nonumber 
\end{eqnarray}
\end{itemize}





\end{frame}

\begin{frame}

\begin{thm}
Suppose $\hat{f}(x)$ is a linear smoother.  Then CV(b) can be written as 

\begin{eqnarray}
\text{CV(b)} & = & \frac{1}{N} \sum_{i=1}^{N}\left( \frac{Y_{i} - \hat{f}_{b} (x)}{1 - H_{ii}}  \right)^2 \nonumber 
\end{eqnarray}

And is often approximated with the Generalized Cross Validation (GCV): 

\begin{eqnarray}
\text{GCV(b)} & = & \frac{1}{N} \sum_{i=1}^{N}\left( \frac{Y_{i} - \hat{f}_{b} (x)}{1 - \nu/N}  \right)^2 \nonumber 
\end{eqnarray}
where $\nu = \text{tr}(\boldsymbol{H})$

\end{thm}



\end{frame}


\begin{frame}


\only<1>{\scalebox{0.45}{\includegraphics{CV1.pdf}}}

\only<2>{\scalebox{0.45}{\includegraphics{CV2.pdf}}}



\end{frame}

\begin{frame}

\begin{columns}

\column{0.5\textwidth}

\scalebox{0.45}{\includegraphics{CV3.pdf}}


\column{0.4\textwidth}
{\tt qplot(lwt, bwt, geom='point') + stat\_smooth(degree = 1, span = 0.83) } 

\end{columns}
\end{frame}




\begin{frame}
\frametitle{Loess and Conditioning} 
Many models:\pause \invisible<1>{ condition on \alert{many} factors in order to learn relationship} \pause \\
\invisible<1-2>{Possible to visualize relationships in \alert{lattice}:} \pause \\
\invisible<1-3>{Suppose we have continuous random variable $X$ and discrete random variables $Z_{1}$ and $Z_{2}$.} \pause \\
\invisible<1-4>{Then we might estimate} \pause  \\

\begin{eqnarray}
\invisible<1-5>{f(x, z_{1}, z_{2}) & = & E[Y|X=x, Z_{1} =z_{1}, Z_{2} = z_{2} ] \nonumber} \pause  \\
\invisible<1-6>{& = &  \int_{-\infty}^{\infty} y f(y|X=x, Z_{1} =z_{1}, Z_{2} = z_{2} ) dy \nonumber}  
\end{eqnarray}



\end{frame}


\begin{frame}
\frametitle{Loess and Conditioning} 

\begin{columns}[]
\column{0.6\textwidth} 
\scalebox{0.5}{\includegraphics{FinalFig.pdf}}

\column{0.4\textwidth}
$<$ R Code $>$
{\tt xyplot(bwt$\sim$lwt|race*smoke, lwd=4, col='cornflowerblue', xlab='Mother's Weight', ylab='Birth Weight',\\
	panel= function(x, y, ...)\{\\
		panel.xyplot(x, y, ...); panel.loess(x,y,span=.8, degree=1, ...)
		\}
		)
		}
\end{columns} 

\end{frame}


\begin{frame}
\huge
Densities

\end{frame}



\begin{frame}
\frametitle{Histogram, Probability Mass Function} 

Simplest case: \alert{probability mass functions} \pause \\
\begin{itemize}
\invisible<1>{\item[-] Suppose we observe $x_{1}, x_{2}, \hdots, x_{N}$} \pause 
\invisible<1-2>{\item[-] If $X$'s takes on few unique values, say $K<<N$, then an estimator for the pmf is, } \pause 
\end{itemize}
\begin{eqnarray}
\invisible<1-3>{\widehat{p}(X = x) \equiv \widehat{p}(x) & = & \frac{ \sum_{i=1}^{N} I(x_{i} =x) }{N}  \nonumber } \pause 
\end{eqnarray}
\invisible<1-4>{Will converge on the probability mass function (by weak law of large numbers)}  \\


\end{frame}


\begin{frame}
\frametitle{Histogram, Probability Density Function}


Harder case: \alert{probability density functions} 
\begin{itemize}
\item[-] Suppose we observe $x_{1}, x_{2}, \hdots, x_{N}$, with $X$'s continuous
\item[-] Goal: estimate the pdf $f(x)$.  
\item[-] Define a set of bins $b_{1}, b_{2}, \hdots , b_{M}$ such that 
\begin{itemize}
\item[-] $b_{1}\leq x_{1}$ and $b_{M}\geq x_{N}$
\end{itemize}
\item[-] Then, we can define an estimator for the pdf $f(x)$ as
\end{itemize}
\begin{eqnarray}
\widehat{f(x)} & = & \frac{\sum_{i=1}^{N} I(x_{i}>b_{z} \text{ and } x_{i}\leq b_{z+1}) }{N}\times I(x \in (b_{z}, b_{z+1}]) \nonumber 
\end{eqnarray}

In words: $\widehat{f(x)}$ is equal to proportion of observations in the bandwidth that $x$ resides in

\end{frame}


\begin{frame}
\frametitle{Bias Variance Tradeoff on Bin Size}

\begin{center}
\only<1>{\scalebox{0.5}{\includegraphics{hist1.pdf}}}
\only<2>{\scalebox{0.5}{\includegraphics{hist2.pdf}}}
\only<3>{\scalebox{0.5}{\includegraphics{hist3.pdf}}}
\only<4>{\scalebox{0.5}{\includegraphics{hist4.pdf}}}
\only<5>{\scalebox{0.5}{\includegraphics{hist5.pdf}}}
\only<6>{\scalebox{0.5}{\includegraphics{hist6.pdf}}}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Bias-Variance Tradeoff/Guidance}

Classic problem: \\
\alert{Bias-Variance} Tradeoff


\begin{center}
\only<1>{\scalebox{0.5}{\includegraphics{BiasVariance1.pdf}}} 
\only<2>{\scalebox{0.5}{\includegraphics{BiasVariance2.pdf}}} 
\only<3>{\scalebox{0.5}{\includegraphics{BiasVariance3.pdf}}}
\end{center}



\end{frame}


\begin{frame}
\frametitle{Navigating the Bias Variance Tradeoff}

Two formulas for navigating bias-variance tradeoff for data $x$


\begin{itemize}
\item[1)] width$ = \frac{3.5 \times \text{sd}(x)}{n^{1/3}} $  (Scott 1979) 
\item[2)] width$ = \frac{2 (Q_{3} - Q_{1}) }{n^{1/3} } $ (Freedman and Diaconis (1981) )
\end{itemize}


\end{frame}


\begin{frame}
\frametitle{Histogram Creation}

$<$ R Code $>$\\
\only<1>{\hspace{0.25in} {\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother's Age Histogram', col='gray', border='white')

}}


\only<2>{\hspace{0.25in} {\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother's Age Histogram', col='gray', border='white', nbins='fd')

}}

\only<3>{\hspace{0.25in} {\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother's Age Histogram', col='gray', border='white', nbins='scott')}}


\begin{center}
\only<1>{\scalebox{0.4}{\includegraphics{histEst1.pdf}}} 
\only<2>{\scalebox{0.4}{\includegraphics{histEst2.pdf}}}
\only<3>{\scalebox{0.4}{\includegraphics{histEst3.pdf}}}
\end{center}




\end{frame}



\begin{frame}
\frametitle{Density Estimation}

Histograms make continuous data discrete\\
\alert{Density} estimators ``smooth" out histograms.\\
We will use Kernels again


\end{frame}


\begin{frame}
\frametitle{Gaussian Kernel}

Define
\begin{eqnarray}
\only<1>{K(x) & = & \frac{\exp[ -x^2 ] }{\sqrt{2\pi} } \nonumber }
\only<2->{K(\frac{ x - x_{j} }{b}) &=& \frac{\exp[ -\left(\frac{ x - x_{j} }{b} \right)^2 ] }{\sqrt{2\pi} }\nonumber  } 
\end{eqnarray}


\begin{center}
\only<1>{\scalebox{0.35}{\includegraphics{Kernel1.pdf}}}
\only<2>{\scalebox{0.35}{\includegraphics{Kernel2.pdf}}}
\only<3>{\scalebox{0.35}{\includegraphics{Kernel3.pdf}}}
\only<4>{\scalebox{0.35}{\includegraphics{Kernel4.pdf}}}

\end{center}



\end{frame}


\begin{frame}
\frametitle{Density Estimation}

For all values $x \in \Re$ define the kernel density estimator as, 
\begin{eqnarray}
\widehat{f}(x) & = & \frac{1}{n b} \sum_{j=1}^{n} K(\frac{x - x_{j}  }{b} ) \nonumber 
\end{eqnarray}

In words: at each point $x$, calculate weighted average of points, where  weight is given by kernel
\begin{itemize}
\item[-] As $b$ increases, weight on nearby points is more evenly distributed (redistributed away from $x$)
\item[-] As $b$ decreases, weight on nearby points is more concentrated (redistributed towards $x$ )
\end{itemize}


\alert{Another bias variance tradeoff!}

\end{frame}


\begin{frame}
\frametitle{Density Estimation} 

\only<1>{\hspace{0.25in}{\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother\'s Age Histogram', col='gray',  border='white', nbins='scott')\\
\hspace{0.25in} lines(density(birthwt\$age, width=40), lwd=3) }}

\only<2>{\hspace{0.25in}{\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother\'s Age Histogram', col='gray',  border='white', nbins='scott')\\
\hspace{0.25in} lines(density(birthwt\$age, width=10), lwd=3) }}


\only<3>{\hspace{0.25in}{\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother\'s Age Histogram', col='gray',  border='white', nbins='scott')\\
\hspace{0.25in} lines(density(birthwt\$age, width=1), lwd=3) }}


\only<4>{\hspace{0.25in}{\tt truehist(birthwt\$age, xlab='Mother\'s age', ylab='Density', main='Mother\'s Age Histogram', col='gray',  border='white', nbins='scott')\\
\hspace{0.25in} lines(density(birthwt\$age, width='SJ-dpi'), lwd=3) }}



\only<1>{\scalebox{0.4}{\includegraphics{Density2.pdf}}}
\only<2>{\scalebox{0.4}{\includegraphics{Density3.pdf}}}
\only<3>{\scalebox{0.4}{\includegraphics{Density4.pdf}}}
\only<4>{\scalebox{0.4}{\includegraphics{Density5.pdf}}}





\end{frame}



\begin{frame}
\frametitle{Back to the Birth Weight}

\only<1-3>{An initial inference:\pause \\
\invisible<1>{Compare (contrast):} \pause 
\begin{eqnarray}
\invisible<1-2>{\widehat{p}(y|\text{white} ) & \text{ and } & \widehat{p}(y|\text{black} ) \nonumber } \pause 
\end{eqnarray}
}

\begin{columns}[]
\column{0.6\textwidth} 
\only<4>{\invisible<1-3>{\scalebox{0.5}{\includegraphics{QQBW.pdf}}} } 
\column{0.4\textwidth}
\only<4>{ qqplot( \\bwt[which(race2=='white')], bwt[which(race2=='black')], xlab='White Birth Weight (grams)', ylab='Black Birth Weight (grams)', frame.plot=F, main='Comparing Birth Weight across Racial Groups', pch=20)\\
arrows(0,0, 1e7, 1e7)}
\end{columns}

\end{frame}



\begin{frame}
\frametitle{Box Plot}
\only<1-3>{\begin{itemize}
\invisible<1>{\item[-] QQ-plots compare two distributions directly} 
\invisible<1-2>{\item[-] Often want to compare many distributions (and readers are uncomfortable with qq-plots)} 
\end{itemize}}

\pause \pause \pause 
\begin{columns}[]
\column{0.6\textwidth}
\invisible<1-3>{\scalebox{0.5}{\includegraphics{BoxPlot.pdf}}} 
\column{0.4\textwidth}
\invisible<1-3>{$<$ R Code $>$
{\tt library(lattice) }\\
{\tt bwplot(race2 $\sim$bwt, xlab='Birth Weight (grams)', frame.plot=F,main = 'Birthweight and Race', 
\hspace{0.25in} panel=function(...)\{
	panel.bwplot(..., fill='gray', lty=1, lwd=2, pch=20)
	\}
	)
	}
	}
\end{columns}



\end{frame}

\begin{frame}
\frametitle{Violin Plot} 
\pause 

\only<1-3>{\begin{itemize}
\invisible<1>{\item[-] Box plots can obscure variation in distributions [histogram]}  
\invisible<1-2>{\item[-] \alert{Violin plots} allow smooth estimates of data distribution [density]} 
\end{itemize}}



\begin{columns}[]
\column{0.6\textwidth} 
\only<1-4>{\invisible<1-3>{\scalebox{0.5}{\includegraphics{ViolinPlotb.pdf}}} }
\only<5>{\scalebox{0.5}{\includegraphics{ViolinPlotc.pdf}}}


\column{0.4\textwidth}
\invisible<1-3>{$<$ R Code $>$
{\tt library(lattice)} \\
{\tt bwplot(race2$\sim$bwt, xlab='Birth Weight (grams)', frame.plot=F, main='Birth Weight and Race', 
	panel=function(...)\{
		panel.violin(..., col='gray', bw='SJ-dpi')
		\}
	)
}
}

\end{columns}


\pause \pause \pause 
\end{frame}



\begin{frame}
\frametitle{Box (Violin) Plot with Conditioning}
\only<1-5>{\begin{itemize}
\item[-] Comparing distributions within \alert{strata}  (buckets) 
\begin{itemize}
\invisible<1>{\item[-] Here: compare distribution of birth weights across race, by smoker and non-smoker} 
\end{itemize}
\invisible<1-2>{\item[-] Formally: } 
\end{itemize}
\begin{eqnarray}
\invisible<1-3>{\widehat{p}(y| \text{black, smoke} )}  \invisible<1-4>{&;& \widehat{p}(y| \text{black, non-smoke} )\nonumber }\\
 \invisible<1-3>{\widehat{p}(y| \text{white, smoke} )} \invisible<1-4>{&;& \widehat{p}(y| \text{white, non-smoke} )\nonumber }\\
 \invisible<1-3>{\widehat{p}(y| \text{other, smoke} )} \invisible<1-4>{&;& \widehat{p}(y| \text{other, non-smoke} )\nonumber} 
\end{eqnarray}}
\pause \pause \pause \pause 


\begin{columns}[]
\column{0.6\textwidth}
\only<6>{\scalebox{0.5}{\includegraphics{BoxPlot2.pdf}}}
\only<7>{\scalebox{0.5}{\includegraphics{BoxPlot3.pdf}}}
\column{0.4\textwidth}
\only<6>{
{\tt library(lattice) } 
{\tt bwplot(race2$\sim$bwt|smoke, \\
xlab='Birth Weight (grams)', frame.plot=F,main = 'Birth Weight and Race, Given Smoking') } 
}
\only<7>{
{\tt library(lattice) } 
{\tt bwplot(race2$\sim$bwt|smoke,\\
xlab='Birth weight', panel=function(...)\{
	panel.violin(..., col='gray',bw='SJ-dpi')
	\}
	)
	}
	}
\end{columns}


\end{frame}


\begin{frame}

Nonparametrics: Bias/Variance Tradeoff

\end{frame}


\end{document}
